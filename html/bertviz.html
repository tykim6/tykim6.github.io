<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>BERT Attention Visualization</title>
    <link rel="stylesheet" href="../style.css" />
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet" />
</head>

<body>
    <div class="topnav">
        <a href="../index.html">Home</a>
        <a href="projects.html">Projects</a>
        <a href="resume.html">Resume</a>
    </div>

    <h1 class="page-header">BERT Attention Visualization</h1>

    <div class="project-media">
        <h2>Self-Attention Patterns</h2>
        <iframe src="../project_files/bertviz_head_view.html" width="100%" height="800px"></iframe>

        <h2>Head-to-Head Attention Analysis</h2>
        <iframe src="../project_files/bertviz_model_view_algae.html" width="100%" height="800px"></iframe>

        <p>
            This visualization demonstrates attention patterns in BERT (Bidirectional Encoder Representations from
            Transformers).
            The visualizations show how different attention heads focus on various parts of the input text,
            helping us understand how BERT processes and understands language.
        </p>
    </div>
</body>

</html>